{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727b38e3-4bf1-469b-bbdf-1c3b01079f22",
   "metadata": {},
   "source": [
    "# Salary estimator from listings\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we will use the [LinkedIn Job Postings (2023 - 2024)](https://www.kaggle.com/datasets/arshkon/linkedin-job-postings) kaggle dataset to estimate job pay based on a job title and the state it is in. As we analyze the data, we will use the [Plotly](https://plotly.com/python/) library for visualizations. We will train the extreme gradient boosted decision trees machine learning algorithm to estimate the job pay. We will use the XGBRegressor model from the [DMLC XGBoost](https://xgboost.readthedocs.io/) library. That XGBoost model only allows numeric inputs. We will use the [Sci-kit Learn](https://scikit-learn.org/) library to pipe our data through transformers that translate the job titles and states into numbers and interact with XGBoost. We will use the Sci-kit Learn OneHotEncoder to transform the state. Since the job titles have such a high cardinality, we need to represent them differently. So, we will use the [Gensim](https://radimrehurek.com/gensim/) library Word2Vec model to create word embedding vectors representing the job titles for XGBoost. After training XGBRegressor, we will use Ipython to make a small user interface to estimate the job pay based on the job title and state.\n",
    "\n",
    "Click `Runtime` > `Run all` or press `ctrl` + `F9`. Then click `Run anyway.` On Google Colab, this usually takes around five minutes.\n",
    "\n",
    "You can skip to where it makes estimations or go through each section to understand what is happening in the application. If you want to skip to the estimation section, scroll to the bottom block to read the instructions and use the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8472dd",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To begin, we will install all the necessary packages for the application to run. We can do that by running the block of code below that uses IPython commands to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a91ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"scipy==1.12\"  gensim xgboost scikit-learn pyarrow plotly Jinja2 nbformat ipywidgets pandas gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b1b68b",
   "metadata": {},
   "source": [
    "Next, we retrieve the data and the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bad621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_path = os.path.abspath('.')    \n",
    "import shutil\n",
    "\n",
    "\n",
    "def rm_files(root, dir_path):\n",
    "    for x in os.listdir(dir_path):\n",
    "        xname = '/'+x\n",
    "        if os.path.isfile(dir_path+xname):\n",
    "            os.remove(dir_path+xname)\n",
    "        elif x != '.config' and os.path.isdir(dir_path+xname):\n",
    "            rm_files(root, dir_path+xname)\n",
    "    if root != dir_path:\n",
    "        os.rmdir(dir_path)\n",
    "\n",
    "\n",
    "def extract_files(root, dir_path):\n",
    "    for x in os.listdir(dir_path):\n",
    "        xname = '/'+x\n",
    "        if root != dir_path and x[0] != '.':\n",
    "            shutil.move(dir_path+xname, root+xname)\n",
    "        elif os.path.isdir(dir_path+xname):\n",
    "            rm_files(root, dir_path+xname)\n",
    "        else:\n",
    "          os.remove(dir_path+xname)\n",
    "\n",
    "\n",
    "if not os.path.exists(dir_path+'/settings.py'):\n",
    "    !git clone --depth 1 https://github.com/Trail3lazer/pay-estimator.git\n",
    "    import gdown\n",
    "    url = 'https://drive.google.com/drive/folders/1ZTZDh1TPjUOBy7sAruyIt567GFDHQ2C-?usp=sharing'\n",
    "    output = dir_path+'/pay-estimator'\n",
    "    gdown.download_folder(url=url, output=output, quiet=False)\n",
    "    extract_files(dir_path, output)\n",
    "    os.rmdir(output)\n",
    "    \n",
    "    \n",
    "import settings\n",
    "if not os.path.exists(settings.APP_ARCHIVE_PATH):\n",
    "    os.makedirs(settings.APP_ARCHIVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8261e7",
   "metadata": {},
   "source": [
    "## Parsing and cleaning the data\n",
    "\n",
    "After we set up our supporting code and data, we can start parsing the data! First, we will instantiate the DataManager class. It will handle reading the CSV files, joining the data, dropping unnecessary columns, renaming confusing columns, parsing which state the jobs are in, getting the average pay for each job, transforming different pay periods to yearly pay for consistency, dropping duplicates, and saving the cleaned, transformed job postings.\n",
    "\n",
    "Run the code block below to create the DataManager class and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import DataManager\n",
    "\n",
    "dm = DataManager()\n",
    "df = dm.get_postings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d4f2d",
   "metadata": {},
   "source": [
    "Now that we have cleaned the data, we will review a few postings. We will shorten some text columns to fit most of the rows on the screen vertically. You can use the horizontal scroll bar to see more columns.\n",
    "\n",
    "Run the code block below to see a sample set of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "df = dm.get_postings()\n",
    "def shorten_long_cols(row):\n",
    "    for name in ['job_desc','company_desc','skills_desc']:\n",
    "        if isinstance(row[name], str):\n",
    "            row[name] = row[name][:150] + '...' \n",
    "    return row\n",
    "\n",
    "display(HTML(df.sample(2).apply(shorten_long_cols, axis=1).to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195ec2c",
   "metadata": {},
   "source": [
    "There are a few important items we should pay attention to. \n",
    "\n",
    "1. The location column, which is the location of the job, is not normalized to a state abbreviation. Usually, it's in the format \"City, State,\" but that is not always the case. The state column originally represented the location of the company. Many jobs are in states other than where the company HQ resides. So we use some regex and code in the DataManager to extract the state and translate it to a state abbreviation, then save it to the state column. We use the company state column if we can't extract a state from the location column. \n",
    "\n",
    "2. Job titles are generally unique. Their uniqueness could cause some issues when we try to use regression to estimate salaries for job titles. If the XGBoost model receives data with high cardinality, it will be much less accurate. So, we need to create word embeddings to represent the job titles and reduce their complexity. We will use the Word2Vec model to generate vectors from words and use the vectors to represent the job titles with XGBoost.\n",
    "\n",
    "3. To ensure the accuracy of the vectors, we will use a substantial amount of text data, including The job_title, job_desc, job_skills, skills_desc, and company_desc, to train the word2vec model. We need a lot of data to make the vectors, so we can use the plethora of text from these columns to train it.\n",
    "\n",
    "4. The pay_period and salary columns include data for different pay periods. If we train the model with inconsistent pay information, it will not be able to estimate pay accurately. We needed to normalize those columns. So, the DataManager class converts all pay periods to yearly pay. For most pay periods, it multiplies the pay to represent a year's pay. e.g., monthly pay times twelve equals annual pay. We must account for time off, full-time/part-time hours, and holidays for hourly pay. The DataManager uses statistics from the Bureau of Labor Statistics to calculate the average full-time and part-time working hours per week and working weeks per year. Then, it multiplies those with the pay to create the job's yearly pay.\n",
    "\n",
    "5. It also takes the average between the max, med, and min salary columns if they exist. The salary columns having _from_salaries at the end of their names come from the salaries CSV file. So, if the primary salary columns are empty, the DataManager uses the backup columns to calculate the pay. After normalizing the pay columns, the DataManager saves the results to the \"pay\" column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b9edbd",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Run the code block below to create descriptive statistics for the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_cols = ['max_salary','med_salary','min_salary','pay']\n",
    "pay_period_df = dm.get_postings()[pay_cols]\n",
    "desc = pay_period_df.describe()\n",
    "\n",
    "message = '''\n",
    "<div style=\"font-size:17px\">\n",
    "    <p>\n",
    "        The pay statistics have a few interesting features. \n",
    "        First, the \"pay\" column is a calculated column the DataManager created earlier. \n",
    "        It represents the average for all pay columns in each row.\n",
    "    </p>\n",
    "'''\n",
    "for col in pay_cols:\n",
    "    message += r'''<p>\n",
    "    There are {count:,.0f} {col} values. The average {col} is &dollar;{mean:,.0f}, and the standard deviation is {std:,.0f}. \n",
    "    The {col} column has a minimum of &dollar;{min:,.0f}, a median of &dollar;{50%:,.0f}, and a maximum of &dollar;{max:,.0f}. \n",
    "    25&percnt; of the {col} values are less than &dollar;{25%:,.0f}, and 25&percnt; are greater than &dollar;{75%:,.0f}. \n",
    "    So, 50&percnt; of the values are between &dollar;{25%:,.0f} and &dollar;{75%:,.0f}.\n",
    "    </p>'''.format(col=col, **desc[col])\n",
    "\n",
    "message += '''<p>\n",
    "It's important to note that the column values are not all filled out. \n",
    "We have more max and min salary entries than med salary entries. \n",
    "This difference suggests potential data gaps that need further investigation. \n",
    "Additionally, many jobs are missing pay information, as the number of rows with pay values is significantly lower than the total number of jobs.\n",
    "</p></div>'''\n",
    "display(HTML(desc.style.format(precision=0,thousands=\",\").to_html()))\n",
    "display(HTML(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a200b6",
   "metadata": {},
   "source": [
    "Next, we will create some box pots to visualize the distribution of the pay columns. We will use a graphing library called Plotly to display our data.\n",
    "\n",
    "Run the code block below to create the boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4078bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(layout=dict(height=800, title='Job Posting Pay Column Box Plots'))\n",
    "fig.add_traces([go.Box(y=pay_period_df[col], boxpoints=False, name=col) for col in pay_cols])\n",
    "fig.update_yaxes(tickprefix = '$', tickformat = ',.0f', type=\"log\",automargin=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f8b72a",
   "metadata": {},
   "source": [
    "The box plots above represent the descriptive statistics we generated earlier.\n",
    "\n",
    "Next, we will create a bar graph of average yearly pay by state. Run the code block below to generate the bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f392e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "df = dm.get_postings()[['state','pay']].copy().dropna(how='any')\n",
    "\n",
    "groups = df.groupby('state')\n",
    "group_count = groups.count()\n",
    "df = groups.mean()\n",
    "df['count'] = group_count\n",
    "df = df.dropna(axis=1).sort_values(by='pay')\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x = df.index.values, \n",
    "        y=df['pay'],\n",
    "        name=\"Average Yearly Pay\",\n",
    "    ), \n",
    "    secondary_y=False)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x = df.index.values,\n",
    "        y = df['count'],\n",
    "        name=\"Sample Size\"\n",
    "    ),\n",
    "    secondary_y=True\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text=\"Average Job Posting Pay By State\"),\n",
    "    xaxis=dict(title_text=\"State\",tickangle=90),\n",
    "    yaxis=dict(title_text=\"Average Yearly Pay\")\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Job Listings (log)\", secondary_y=True, type=\"log\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22687285",
   "metadata": {},
   "source": [
    "The bars represent the average yearly pay in each state. The red line represents the number of job listings for that state. Some states have very few job listings. Our pay estimates in those states will be less accurate. Specifically, Wyoming has the least job listings at 96.\n",
    "\n",
    "Additionally, the states with the most listings will skew the overall dataset statistics. California has the most listings at 6615. The state with the minimum average pay is Mississippi, with an average income of &dollar;56,768.58. Washington, DC, has the highest average income at &dollar;111,351.10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff470ef",
   "metadata": {},
   "source": [
    "## Word embedding model\n",
    "\n",
    "Now, we should see what the average pay is for different jobs. Since the job titles have a high cardinality, we can not reasonably display them on an axis in a chart. So before we try to get the averages, we need to sort the job postings into categories. To do that, we will create word embeddings with Word2Vec. The class called Job2Vec prepares training data and trains and manages the Word2Vec model.\n",
    "\n",
    "Run the code block below to create the Job2Vec class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1cc27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordmod import Job2Vec\n",
    "\n",
    "job2vec = Job2Vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78d5db",
   "metadata": {},
   "source": [
    "It takes a while to train the Word2Vec model, so I saved it in the assets folder. We will reuse the trained model if possible.\n",
    "\n",
    "If we have to train a new model, we will tokenize all the text data and then pass it to the Job2Vec class to train it. The Job2Vec class will save the tokenized strings to the \"archive/app\" folder because tokenizing the text takes a long time. \n",
    "\n",
    "Run the code block below to get or train the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Loading j2v word vectors.')\n",
    "j2v = job2vec.try_get_model()\n",
    "\n",
    "if j2v is None:\n",
    "    dataset = job2vec.try_load_dataset()\n",
    "\n",
    "    if dataset is None:\n",
    "        df = dm.get_postings().copy()\n",
    "\n",
    "        print(\"Combining the the bls.gov job list, LinkedIn job title, description and skills, columns, and other tables to create a single array. Word2Vec does not need them separated.\")\n",
    "        bls = dm.get_bls_jobs().to_numpy()\n",
    "        others = np.concatenate(dm.load_additional_tables())\n",
    "        data = [bls, others, df['job_title'].unique(), df['job_desc'].unique(), df['skills_desc'].unique(), df['company_desc'].unique()]\n",
    "        ser = np.concatenate(data)\n",
    "\n",
    "        dataset = job2vec.preprocess_data(ser)\n",
    "    \n",
    "    j2v = job2vec.try_get_model(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbaa874",
   "metadata": {},
   "source": [
    "Now that the word embedding model is loaded and trained, we can use the word vectors to create categories and categorize the jobs. I made a categorizer class that will take a list of categories and create vectors for each category. Then, we can use the original model to vectorize job titles and use the category vectors to determine the most similar category to the job title. First, we will instantiate the Categorizer class.\n",
    "\n",
    "Run the code block below to create the Categorizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76984b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catword import Categorizer\n",
    "import pandas as pd\n",
    "\n",
    "categorizer = Categorizer(j2v.wv, job2vec.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003b78d",
   "metadata": {},
   "source": [
    "The data manager can use the categorize function from the categorizer class to categorize each job title and save the result to a category column. So, we will have the DataManager categorize our job titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde119e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dm.get_or_create_categorized_postings(categorizer.categorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7afea07",
   "metadata": {},
   "source": [
    "## More Data Analysis\n",
    "\n",
    "Now that the job titles are classified, we will generate another bar chart to show the average pay for each job category. Run the code block below to create the job category pay bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df[['pay', 'category']].dropna(how='all').groupby('category')\n",
    "fig_df = groups.mean(numeric_only=True).sort_values(by='pay')\n",
    "fig_df['count'] = groups.count()\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig = fig.add_trace(go.Bar(x=fig_df.index.values, y=fig_df['pay'], name='Average Pay'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text=\"Average Job Posting Pay By Category\"),\n",
    "    yaxis=dict(title_text=\"Average Yearly Pay\")\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x = fig_df.index.values,\n",
    "        y = fig_df['count'],\n",
    "        name=\"Sample Size\"\n",
    "    ),\n",
    "    secondary_y=True\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Job Listings (log)\", secondary_y=True, type=\"log\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0370d0a",
   "metadata": {},
   "source": [
    "This bar chart is laid out similarly to the Average Job Posting Pay by State chart. The red line shows the number of jobs in each category. The category with the most jobs is business, with 6482 job listings. The category with the fewest listings is environment, with 48 listings. The periwinkle bars show the average pay for each category. The category with the highest average salary is information technology. The categories with the lowest average pay are retail and grocery.\n",
    "\n",
    "Now that we have all the jobs categorized, we can create a heat map that shows which states have the most and least average pay for each category. Some states do not have jobs in different categories, so those squares are blank. The more yellow a square is, the higher its average pay.\n",
    "\n",
    "Run the code block below to create the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49786096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df = dm.get_or_create_categorized_postings(categorizer.categorize).copy()\n",
    "fig_df = df.copy()\n",
    "fig_df = fig_df[['category','state','pay']]\n",
    "fig_df = fig_df.groupby(by=['category','state'], group_keys=False).mean().round(2)\n",
    "fig_df.name = 'pay'\n",
    "heat_df = fig_df.unstack(level=1)\n",
    "heat_df.columns = heat_df.columns.droplevel(0)\n",
    "fig = px.imshow(heat_df, aspect=\"auto\", height=700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4057f",
   "metadata": {},
   "source": [
    "This heat map represents the average pay for each job category in each state. The more yellow a cell is, the higher the average salary is. Conversely, if a cell is more purple, it represents lower pay. Interestingly, some blocks on the map are empty. The state has no listings in that job category if the blocks are empty. It's interesting to see some columns and rows are darker than others. It's another representation of what we saw earlier, where some states and job categories pay better than others.\n",
    "\n",
    "Next, we will create a 3D scatter plot of the same data points. It will be fun to visualize these same ideas in a different way. \n",
    "\n",
    "Run the code block below to create the 3D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fig_df.reset_index()\n",
    "df['color'] = 0\n",
    "df.loc[df['pay'] >= 200_000,'color'] = 200_000\n",
    "df.loc[(200_000 > df['pay']) & (df['pay'] >= 150_000),'color'] = 150_000\n",
    "df.loc[(150_000 > df['pay']) & (df['pay'] >= 100_000),'color'] = 100_000\n",
    "df.loc[(100_000 > df['pay']) & (df['pay'] >= 50_000),'color'] = 50_000\n",
    "df.loc[50_000 > df['pay'],'color'] = 0\n",
    "\n",
    "fig = px.scatter_3d(df, x='state', y='category', z='pay', color='color')\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    xaxis=dict(nticks=51),\n",
    "    yaxis=dict(nticks=51),\n",
    "    margin={\"t\":0,\"b\":0}\n",
    ")\n",
    "fig.update_scenes(aspectmode='cube')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1c8339",
   "metadata": {},
   "source": [
    "Awesome! This graph is more difficult to read than the heat map, but it's much more fun. You can click and drag your mouse around the graph window, which will rotate. Additionally, you can scroll in the graph window to zoom in and out. If you hover over different markers, you'll see the data they represent. The data points are the same as the heat map, but it's interesting to see the distribution of job pay by the colors and groupings of the points and a 3D space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b5b8c",
   "metadata": {},
   "source": [
    "## XGBoost training\n",
    "\n",
    "Now, we can get to work on the estimation model. We will use the XGBoost XGBRegressor class to predict the pay for different job titles in various states. First, we must filter out all the jobs where the state, job title, or pay are invalid. After we filter the columns, we will create two distinct data frames. One will be our training data, and the other will be our target data. The training data will include the state and job title. The target data will consist of the pay. They are labeled X and Y. Then we can pass those to the Sci-kit Learn train test split method, which will split our data into a training and testing set. We will use a test size of 0.1.\n",
    "\n",
    "Run the code block below to create the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c8569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df = dm.get_postings().copy()\n",
    "x_cols=['state',\n",
    "        'job_title']\n",
    "y_col = 'pay'\n",
    "\n",
    "mask = df[['job_title', 'state', y_col]].notna().all(axis=1) & df[y_col].gt(0)\n",
    "df = df[x_cols+[y_col]].loc[mask].copy().reset_index()\n",
    "\n",
    "x, y = df[x_cols], df[y_col]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb5ed1",
   "metadata": {},
   "source": [
    "After splitting our training data set, we can prepare a pipeline to transform the data for xgboost. XGBoost only accepts numerical data, so we must turn our string values into numbers. We could use a map of numbers to the values. For example, we could map AK to 1, AL to 2, AR to 3, etc. The issue with doing that is that XGBoost may think that the states with the higher numbers are more valuable in some way. That could cause issues where a state like Wyoming, which comes in the end, could have much higher predictions than it should. A common way to solve this is to use One Hot Encoding. One Hot Encoding splits each state into its column and uses one or zero to note if that is the state for the row. For example, let's say we have a row in which the state is CA. Then the row may look like this:\n",
    "\n",
    "| job_title | ... | AR | AZ | CA | CO | ... |\n",
    "| --------- | --- | -- | -- | -- | -- | --- |\n",
    "| \"Some job\"| ... | 0  | 0  | 1  | 0  | ... |\n",
    "\n",
    "Since only 52 states (including DC) exist, One Hot Encoding should work great.\n",
    "\n",
    "With the job titles, we will run into the same problem we had earlier because the job titles have a high cardinality. In other words, there are many different job titles, but there are far too many to split them up meaningfully with One Hot Encoding. We could categorize the jobs like we did earlier, but then the model can only estimate based on job categories. We want it to calculate pay for many jobs, not just job categories. Luckily, the Word2Vec model we created uses numerical vectors to represent words. We can represent job titles by retrieving the vectors for each word in the job title and calculating the average from them. The Job2Vec class will calculate the mean vector for the job titles. We just need to pass the job title to its vectorize method. So, we will create a custom functional transformer that vectorizes the job titles in our dataset.\n",
    "\n",
    "Now that we know what to do with each column, we can create a pipeline that handles each column individually. After we make the column transformer, we must fit the transformer with our training data.\n",
    "\n",
    "Run the code block below to create the data preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "\n",
    "\n",
    "vector_length = j2v.wv.vector_size\n",
    "\n",
    "def title_to_vec(titles: pd.DataFrame):\n",
    "    vector_cols = [f'title{n}' for n in range(vector_length)]\n",
    "    rows = [job2vec.vectorize(x) for x in titles['job_title'].values]\n",
    "    return pd.DataFrame(rows, columns=vector_cols)\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"state\", OneHotEncoder(handle_unknown=\"ignore\"), ['state']),\n",
    "    (\"title\", FunctionTransformer(title_to_vec), ['job_title'])\n",
    "])\n",
    "\n",
    "preprocessor = preprocessor.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ce071",
   "metadata": {},
   "source": [
    "After we fit the preprocessor, we can load or train the XGBoost model. We will use another Sci-kit Learn pipeline to easily manage passing data through our preprocessor and the XGBoost regressor model. We will instantiate the XGBRegressor class and use the gbtree booster, hist tree method, regression squared error objective, and mean average error metric in the hyperparameters. There are additional methods I included. The eta max depth N estimators I have set produced the most accurate results from my testing. Reducing the max depth made the model too simplistic to handle the word vectors and the one hot encoded states. Additionally, adding so many trees via the n_estimators parameter helped fine-tune the model.\n",
    "\n",
    "After we create the pipeline and instantiate the regressor class, we can either load my existing model or train a new one. Training such complex decision trees takes a lot of time, processing, and computing power, even with a GPU. Using the model included instead of retraining a new one.\n",
    "\n",
    "Run the code block below to create and load or train the XGBRegressor model and pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import os, settings\n",
    "\n",
    "xgb_reg: xgb.XGBRegressor = xgb.XGBRegressor(\n",
    "    device='cuda',\n",
    "    booster='gbtree',\n",
    "    tree_method= 'hist',\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='mae',\n",
    "    eta=0.1, \n",
    "    max_depth=20,\n",
    "    early_stopping_rounds=15,\n",
    "    verbosity=0,\n",
    "    n_estimators=500\n",
    ")\n",
    "\n",
    "xgb_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    ('reg', xgb_reg)\n",
    "])\n",
    "\n",
    "if os.path.isfile(settings.XGB_MODEL):\n",
    "    xgb_reg.load_model(settings.XGB_MODEL)\n",
    "    \n",
    "else:\n",
    "    x_test_preprocessed = preprocessor.transform(x_test)\n",
    "    \n",
    "    xgb_pipe = xgb_pipe.fit(\n",
    "        x_train, \n",
    "        y_train, \n",
    "        reg__eval_set=[(x_test_preprocessed, y_test)])\n",
    "    \n",
    "    xgb_reg.save_model(settings.XGB_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480e0f5",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Now that our model is trained and loaded, we must test its accuracy. We can do that using our test set. We'll create a small sample of ten different jobs, their actual pay, and what the model estimates the pay to be, just for our reference to see visually how the model performs. Then, we can have the model predict the entire test set and calculate the mean average percentage error (MAPE) to see its estimation accuracy. The mean average percentage error works well with many regression models and helps normalize error rates. So, the mean average percentage error will tell us approximately how inaccurate the model's salary estimation is. It will cover validation for the Word2Vec and XGBoost models because both play a role in creating the estimation through our pipeline.\n",
    "\n",
    "Run the code block below to test the model and calculate the accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(x_test, columns=['state','job_title'])\n",
    "test['pay'] = y_test\n",
    "sm_test = test.sample(5)\n",
    "res = xgb_pipe.predict(sm_test[x_cols])\n",
    "sm_test['predicted']=res\n",
    "display(HTML(sm_test.style.format(precision=2,thousands=\",\").to_html()))\n",
    "\n",
    "mape = ((test['pay']-xgb_pipe.predict(test[x_cols]))/test['pay']).abs().mean()\n",
    "display(HTML(f'''\n",
    "             <p style=\"font-size:18px;color:orange;font-weight:bold\">\n",
    "                    MAPE = {mape*100:.10f}&percnt;\n",
    "             </p>\n",
    "             '''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e674696",
   "metadata": {},
   "source": [
    "## Try It Out\n",
    "\n",
    "The code below creates a simple UI that allows us to enter job information and get yearly and hourly pay estimates. We can use the UI to type in a job title or description and select the state where we want it to estimate the pay. It also displays words similar to the job title we are entering to help us determine if the model correctly understands what we are typing. That way, if we are trying to estimate the pay for a mental health counselor, but the words that pop up include words similar to legal counsel, it is a good indicator that the model may estimate the incorrect pay. In a case like that, we could slightly change the words we enter for the job title to be more unique to that job. So, instead of a mental health counselor, we could enter a mental health therapist. Sometimes, including additional details about the job helps the model be more accurate.\n",
    "Additionally, since we trained the model using a LinkedIn job posting data set, it won't know about jobs that rarely appear on LinkedIn or are not posted on LinkedIn. The model will incorrectly estimate the pay for those jobs because it is not trained by the data to know the correct pay for those jobs. Some examples of jobs it might not understand could include actor, entrepreneur, or farmer.\n",
    "\n",
    "Run the code block below and try typing different job titles into the UI to determine their pay. Some fun ideas could include 'dietitian,' 'mechanic,' 'school teacher,' '[intern, junior, senior] software engineer,' 'structural engineer,' or 'carpenter.' You can change the state to adjust the pay to a specific location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import json, locale\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, '')\n",
    "\n",
    "states = dict(json.load(open(settings.STATE_ABBR)))\n",
    "state_options = dict([(name, states[name]) for name in states])\n",
    "\n",
    "def estimate_job_salary(Job, State):\n",
    "    similar_words = j2v.wv.similar_by_vector(job2vec.vectorize(Job), topn=5)\n",
    "    similar = ', '.join(w[0] for w in similar_words if w[0] not in Job and w[1] > .6) or 'No close similarities, results will likely be inaccurate.'\n",
    "\n",
    "    result = xgb_pipe.predict(pd.DataFrame({'state':[State], 'job_title':[Job]}))\n",
    "    hourly_pay = locale.currency(dm.salary_to_hourly(result[0], 'HOURLY'), grouping=True)\n",
    "    salary_pay = locale.currency(result[0], grouping=True)\n",
    "    \n",
    "    return display(HTML(fr'''\n",
    "<div style=\"background-color:#ebd5b3;color:black;font-size:17px;font-weight:bold;\">\n",
    "    <p>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <td>Salary pay:</td> \n",
    "                <td>{salary_pay}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Hourly pay:</td> \n",
    "                <td>{hourly_pay}</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </p>\n",
    "    <p>\n",
    "        Similar words: {similar}\n",
    "    </p>\n",
    "</div>\n",
    "                        '''))\n",
    "\n",
    "widgets.interactive(estimate_job_salary, Job='', State=state_options, placeholder='Write the job title here.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6feada",
   "metadata": {},
   "source": [
    "If you ran all the code but missed the instructions, they are right before the code block above.\n",
    "\n",
    "#### Resources\n",
    "\n",
    "Boothe, A. (2023, June 29). Regex for 50 US States - data, code and conversation. Sigpwned.com. https://sigpwned.com/2023/06/29/regex-for-50-us-states/\n",
    "\n",
    "KON, A. (2024, May 3). LinkedIn Job Postings - 2023. Www.kaggle.com. https://www.kaggle.com/datasets/arshkon/linkedin-job-postings\n",
    "\n",
    "Paine, J. (n.d.). A python list of all US state abbreviations. Gist. Retrieved July 14, 2024, from https://gist.github.com/JeffPaine/3083347\n",
    "\n",
    "Plotly, Inc. (2021, September 28). plotly.py. GitHub. https://github.com/plotly/plotly.py\n",
    "\n",
    "Řehůřek, R., & Sojka, P. (2010, May 1). Software Framework for Topic Modelling with Large Corpora. GitHub. https://github.com/piskvorky/gensim\n",
    "\n",
    "scikit-learn. (2019, April 15). Scikit-learn/scikit-learn. GitHub. https://github.com/scikit-learn/scikit-learn\n",
    "\n",
    "U.S. Bureau of Labor Statistics. (2017, October 24). A-Z Index: Occupational Outlook Handbook. Bls.gov. https://www.bls.gov/ooh/a-z-index.htm\n",
    "\n",
    "U.S. Bureau of Labor Statistics. (2018). Average hours employed people spent working on days worked by day of week. Bls.gov. https://www.bls.gov/charts/american-time-use/emp-by-ftpt-job-edu-h.htm\n",
    "\n",
    "U.S. Bureau of Labor Statistics. (2019, March 29). Occupational Employment and Wage Statistics. Bls.gov. https://www.bls.gov/oes/current/oes_stru.htm\n",
    "\n",
    "U.S. Bureau of Labor Statistics. (2021, September 23). Who receives paid vacations? Www.bls.gov. https://www.bls.gov/ebs/factsheets/paid-vacations.htm#:~:text=The%20number%20of%20vacation%20days\n",
    "\n",
    "United States Court of Appeals for the Second Circuit. (2024). Federal Holidays 2024. Www.ca2.Uscourts.gov. https://www.ca2.uscourts.gov/clerk/calendars/federal_holidays.html\n",
    "\n",
    "XGBoost Contributors. (2021). GitHub - dmlc/xgboost at stable. GitHub. https://github.com/dmlc/xgboost/tree/stable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
